# v1.0.0 Apply Mode - Local Research

## Overview

Apply mode is the final step in the recursive cleaner workflow: actually applying the generated cleaning functions to the full dataset to produce cleaned output.

## Current Workflow (v0.9.0)

```python
cleaner = DataCleaner(...)
cleaner.run()  # Generates cleaning_functions.py
```

The workflow currently:
1. Chunks the data
2. Analyzes chunks with LLM
3. Generates cleaning functions
4. Writes `cleaning_functions.py`

**Missing**: Applying those functions to produce cleaned data.

## Target API (from TODO.md)

```python
cleaner.run()  # Generates cleaning_functions.py

# NEW: Apply to full dataset
cleaner.apply(output_path="cleaned_data.jsonl")
```

And CLI:
```bash
recursive-cleaner apply <file> --functions cleaning_functions.py -o cleaned.jsonl
```

## Key Files to Understand

### parsers.py (456 lines)
Handles file loading and chunking. Key functions:
- `load_parquet()` - Load parquet as list of dicts
- `chunk_file()` - Main chunking entry point
- `_chunk_jsonl()`, `_chunk_csv()`, `_chunk_json()` - Format-specific chunking

For apply mode, we need **record-by-record iteration**, not chunking. Key insight:
- JSONL: Stream line by line
- CSV: csv.DictReader iteration
- JSON arrays: Load full array, iterate
- Parquet: pyarrow read_table().to_pylist()
- Text: Single string (no records)

### output.py (198 lines)
Generates `cleaning_functions.py`. Key:
- `generate_clean_data_function()` - Creates the `clean_data()` entrypoint
- All generated files have `clean_data(data)` function that applies all cleaning functions in order

This is crucial: **we don't need to know individual functions** - just import `clean_data` from the generated file.

### cleaner.py (770 lines)
Main DataCleaner class. Key state:
- `self.output_path` - Where to write cleaning functions
- `self.functions` - List of generated function dicts
- `self._effective_mode` - "structured" or "text"

### cli.py (348 lines)
Current commands: `generate`, `analyze`, `resume`

New command needed: `apply`

## Design Decisions

### 1. Standalone vs Post-run

**Option A**: `cleaner.apply()` method after `run()`
```python
cleaner.run()
cleaner.apply("cleaned.jsonl")
```
Pros: Integrated workflow
Cons: Requires keeping cleaner instance alive, functions in memory

**Option B**: Standalone function that imports from file
```python
from recursive_cleaner.apply import apply_cleaning
apply_cleaning("data.jsonl", "cleaning_functions.py", "cleaned.jsonl")
```
Pros: Decoupled, can apply any functions file to any data
Cons: Requires file import (security consideration)

**Recommendation**: Option B - Standalone function. Reasons:
1. Matches CLI use case (`apply` is separate command)
2. Allows applying functions days/weeks later
3. Simpler - no need to modify DataCleaner state

### 2. File Format Support

| Input Format | Output Format | Approach |
|--------------|---------------|----------|
| JSONL | JSONL | Stream line-by-line, write cleaned lines |
| CSV | CSV | DictReader → apply → DictWriter with header |
| JSON array | JSON array | Load all, apply each, write array |
| Parquet | Parquet or JSONL | Load all, apply each, write |
| Text | Text | Load all, apply as single string |

**Recommendation**: Output same format as input (or allow override)

### 3. Memory Efficiency

For large files:
- **JSONL**: Can stream (read line, clean, write line, discard)
- **CSV**: Can stream
- **JSON arrays**: Must load full array (JSON constraint)
- **Parquet**: Must load full table (Parquet constraint)
- **Text**: Single record anyway

**Recommendation**: Stream where possible (JSONL, CSV), batch for others.

### 4. Progress Callbacks

Same pattern as `run()`:
```python
def on_progress(event):
    if event["type"] == "apply_progress":
        print(f"{event['records_processed']}/{event['total_records']}")
```

Events:
- `apply_start` - total_records (if known)
- `apply_progress` - records_processed
- `apply_complete` - total_records, output_path

### 5. Importing Generated Functions

The generated file is valid Python with a `clean_data(data)` function. Options:

**Option A**: Dynamic import with importlib
```python
import importlib.util
spec = importlib.util.spec_from_file_location("cleaning_module", functions_path)
module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(module)
clean_data = module.clean_data
```
Pros: Standard approach
Cons: Executes all code in file at import

**Option B**: AST extraction + exec
```python
import ast
tree = ast.parse(code)
# Extract clean_data and dependencies
exec(clean_data_code, namespace)
```
Pros: Can extract only what's needed
Cons: More complex, still uses exec

**Recommendation**: Option A - Dynamic import. The file is already validated and generated by us. Simpler code.

### 6. CLI Integration

```bash
recursive-cleaner apply data.jsonl -f cleaning_functions.py -o cleaned.jsonl
```

Arguments:
- `FILE` - Input data file (required, positional)
- `-f/--functions` - Path to cleaning_functions.py (required)
- `-o/--output` - Output file path (default: input with `.cleaned` suffix)
- `--format` - Output format override (optional)

### 7. Error Handling

What if a cleaning function fails on a record?
- **Option A**: Skip record, log warning
- **Option B**: Fail fast
- **Option C**: Configurable

**Recommendation**: Fail fast (Option B) as default. Data cleaning should be deterministic. A failed record indicates a bug in the generated function.

## Implementation Sketch

### New File: `recursive_cleaner/apply.py` (~100 lines)

```python
def load_cleaning_module(functions_path: str):
    """Dynamically import the cleaning functions module."""
    import importlib.util
    spec = importlib.util.spec_from_file_location("cleaning_module", functions_path)
    module = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(module)
    return module

def apply_to_jsonl(input_path, output_path, clean_fn, on_progress=None):
    """Stream-apply cleaning to JSONL file."""
    with open(input_path) as f_in, open(output_path, 'w') as f_out:
        for i, line in enumerate(f_in):
            record = json.loads(line.strip())
            cleaned = clean_fn(record)
            f_out.write(json.dumps(cleaned) + '\n')
            if on_progress:
                on_progress({"type": "apply_progress", "records_processed": i + 1})

def apply_to_csv(input_path, output_path, clean_fn, on_progress=None):
    """Apply cleaning to CSV file."""
    ...

def apply_cleaning(
    input_path: str,
    functions_path: str,
    output_path: str | None = None,
    on_progress: Callable | None = None,
) -> str:
    """
    Apply cleaning functions to a data file.

    Returns: Path to output file
    """
    # Load cleaning module
    module = load_cleaning_module(functions_path)
    clean_fn = module.clean_data

    # Determine output path
    if output_path is None:
        output_path = input_path.rsplit('.', 1)[0] + '.cleaned.' + input_path.rsplit('.', 1)[1]

    # Route by format
    suffix = Path(input_path).suffix.lower()
    if suffix == '.jsonl':
        apply_to_jsonl(input_path, output_path, clean_fn, on_progress)
    elif suffix == '.csv':
        apply_to_csv(input_path, output_path, clean_fn, on_progress)
    ...

    return output_path
```

### CLI Addition: `cli.py` (+50 lines)

```python
def cmd_apply(args) -> int:
    """Handle the apply command."""
    from recursive_cleaner.apply import apply_cleaning

    # Validate files exist
    if not os.path.exists(args.file):
        print(f"Error: Input file not found: {args.file}", file=sys.stderr)
        return 1
    if not os.path.exists(args.functions):
        print(f"Error: Functions file not found: {args.functions}", file=sys.stderr)
        return 1

    def on_progress(event):
        if event["type"] == "apply_progress":
            print(f"  Processed: {event['records_processed']} records")

    try:
        output = apply_cleaning(
            input_path=args.file,
            functions_path=args.functions,
            output_path=args.output,
            on_progress=on_progress,
        )
        print(f"Cleaned data written to: {output}")
        return 0
    except Exception as e:
        print(f"Error: {e}", file=sys.stderr)
        return 3
```

## Line Count Budget

| Component | Lines |
|-----------|-------|
| `apply.py` | ~100 |
| CLI additions | ~50 |
| Tests | ~200 |
| **Total** | ~350 |

Stays within v1.0.0 ~150 line target for core implementation.

## Summary

**Approach**: Standalone `apply_cleaning()` function + CLI `apply` command

**Key decisions**:
1. Import `clean_data()` from generated file dynamically
2. Stream JSONL/CSV, batch JSON/Parquet
3. Same output format as input (default)
4. Fail fast on errors
5. Progress callbacks for large files

**Files to create/modify**:
- NEW: `recursive_cleaner/apply.py` (~100 lines)
- MODIFY: `recursive_cleaner/cli.py` (+50 lines)
- NEW: `tests/test_apply.py` (~200 lines)
